<h1>Image Scrapper from a url</h1>
<h3>Flags used in commands</h3>
<ul>
  <li><b>-nd</b> prevents the creation of a directory hierarchy (i.e. no directories).</li>
  <li><b>-r</b> enables recursive retrieval. See Recursive Download for more information.</li>
  <li><b>-P</b> sets the directory prefix where all files and directories are saved to.</li>
  <li><b>-A</b> sets a whitelist for retrieving only certain file types.</li>
  <li><b>-r -l 1</b>: recursive level 1</li>
  <li><b>-e robots=off</b>: execute command robotos=off as if it was part of .wgetrc file. This turns off the robot exclusion which means you ignore robots.txt and the robot meta tags (you should know the implications this comes with, take care).</li>
<br>
Make the scraper.sh shell script executable: chmod +x scraper.sh
<br><br>
<h2>Image Scrapper from multiple url can be found in Image-scraper-From-Multiple-Site folder</h2>
